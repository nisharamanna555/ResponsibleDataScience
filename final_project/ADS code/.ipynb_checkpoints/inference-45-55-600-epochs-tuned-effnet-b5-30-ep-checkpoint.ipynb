{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://www.kaggle.com/c/osic-pulmonary-fibrosis-progression/discussion/189346\"><h3>Kaggle writeup</h3></a><br><br>\n",
    "<a href=\"https://medium.com/@artkulakov/how-i-achieved-the-1st-place-in-kaggle-osic-pulmonary-fibrosis-progression-competition-e410962c4edc?source=---------2----------------------------\"><h3>Medium writeup</h3></a><br><br>\n",
    "<a href=\"https://github.com/artkulak/osic-pulmonary-fibrosis-progression\"><h3>Github repo</h3></a><br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_kg_hide-output": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: Invalid requirement: '../input/kerasapplications/keras-team-keras-applications-3b180cb'\n",
      "Hint: It looks like a path. File '../input/kerasapplications/keras-team-keras-applications-3b180cb' does not exist.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: Invalid requirement: '../input/efficientnet/efficientnet-1.1.0/'\n",
      "Hint: It looks like a path. File '../input/efficientnet/efficientnet-1.1.0/' does not exist.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install ../input/kerasapplications/keras-team-keras-applications-3b180cb -f ./ --no-index\n",
    "!pip install ../input/efficientnet/efficientnet-1.1.0/ -f ./ --no-index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'cv2'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpydicom\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'cv2'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import pydicom\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "import tensorflow as tf \n",
    "import matplotlib.pyplot as plt \n",
    "import random\n",
    "from tqdm.notebook import tqdm \n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from tensorflow_addons.optimizers import RectifiedAdam\n",
    "from tensorflow.keras import Model\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow.keras.layers as L\n",
    "import tensorflow.keras.models as M\n",
    "from tensorflow.keras.optimizers import Nadam\n",
    "import seaborn as sns\n",
    "from PIL import Image\n",
    "\n",
    "def seed_everything(seed=2020):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    tf.random.set_seed(seed)\n",
    "    \n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "session = tf.compat.v1.Session(config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/train.csv') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Decay (based on EfficientNets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tab(df):\n",
    "    vector = [(df.Age.values[0] - 30) / 30] \n",
    "    \n",
    "    if df.Sex.values[0] == 'male':\n",
    "       vector.append(0)\n",
    "    else:\n",
    "       vector.append(1)\n",
    "    \n",
    "    if df.SmokingStatus.values[0] == 'Never smoked':\n",
    "        vector.extend([0,0])\n",
    "    elif df.SmokingStatus.values[0] == 'Ex-smoker':\n",
    "        vector.extend([1,1])\n",
    "    elif df.SmokingStatus.values[0] == 'Currently smokes':\n",
    "        vector.extend([0,1])\n",
    "    else:\n",
    "        vector.extend([1,0])\n",
    "    return np.array(vector) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = {} \n",
    "TAB = {} \n",
    "P = [] \n",
    "for i, p in tqdm(enumerate(train.Patient.unique())):\n",
    "    sub = train.loc[train.Patient == p, :] \n",
    "    fvc = sub.FVC.values\n",
    "    weeks = sub.Weeks.values\n",
    "    c = np.vstack([weeks, np.ones(len(weeks))]).T\n",
    "    a, b = np.linalg.lstsq(c, fvc)[0]\n",
    "    \n",
    "    A[p] = a\n",
    "    TAB[p] = get_tab(sub)\n",
    "    P.append(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN for coeff prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img(path):\n",
    "    d = pydicom.dcmread(path)\n",
    "    return cv2.resize(d.pixel_array / 2**11, (512, 512))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "class IGenerator(Sequence):\n",
    "    BAD_ID = ['ID00011637202177653955184', 'ID00052637202186188008618']\n",
    "    def __init__(self, keys, a, tab, batch_size=32):\n",
    "        self.keys = [k for k in keys if k not in self.BAD_ID]\n",
    "        self.a = a\n",
    "        self.tab = tab\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "        self.train_data = {}\n",
    "        for p in train.Patient.values:\n",
    "            self.train_data[p] = os.listdir(f'../input/osic-pulmonary-fibrosis-progression/train/{p}/')\n",
    "    \n",
    "    def __len__(self):\n",
    "        return 1000\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = []\n",
    "        a, tab = [], [] \n",
    "        keys = np.random.choice(self.keys, size = self.batch_size)\n",
    "        for k in keys:\n",
    "            try:\n",
    "                i = np.random.choice(self.train_data[k], size=1)[0]\n",
    "                img = get_img(f'../input/osic-pulmonary-fibrosis-progression/train/{k}/{i}')\n",
    "                x.append(img)\n",
    "                a.append(self.a[k])\n",
    "                tab.append(self.tab[k])\n",
    "            except:\n",
    "                print(k, i)\n",
    "       \n",
    "        x,a,tab = np.array(x), np.array(a), np.array(tab)\n",
    "        x = np.expand_dims(x, axis=-1)\n",
    "        return [x, tab] , a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import (\n",
    "    Dense, Dropout, Activation, Flatten, Input, BatchNormalization, GlobalAveragePooling2D, Add, Conv2D, AveragePooling2D, \n",
    "    LeakyReLU, Concatenate \n",
    ")\n",
    "import efficientnet.tfkeras as efn\n",
    "\n",
    "def get_efficientnet(model, shape):\n",
    "    models_dict = {\n",
    "        'b0': efn.EfficientNetB0(input_shape=shape,weights=None,include_top=False),\n",
    "        'b1': efn.EfficientNetB1(input_shape=shape,weights=None,include_top=False),\n",
    "        'b2': efn.EfficientNetB2(input_shape=shape,weights=None,include_top=False),\n",
    "        'b3': efn.EfficientNetB3(input_shape=shape,weights=None,include_top=False),\n",
    "        'b4': efn.EfficientNetB4(input_shape=shape,weights=None,include_top=False),\n",
    "        'b5': efn.EfficientNetB5(input_shape=shape,weights=None,include_top=False),\n",
    "        'b6': efn.EfficientNetB6(input_shape=shape,weights=None,include_top=False),\n",
    "        'b7': efn.EfficientNetB7(input_shape=shape,weights=None,include_top=False)\n",
    "    }\n",
    "    return models_dict[model]\n",
    "\n",
    "def build_model(shape=(512, 512, 1), model_class=None):\n",
    "    inp = Input(shape=shape)\n",
    "    base = get_efficientnet(model_class, shape)\n",
    "    x = base(inp)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    inp2 = Input(shape=(4,))\n",
    "    x2 = tf.keras.layers.GaussianNoise(0.2)(inp2)\n",
    "    x = Concatenate()([x, x2]) \n",
    "    x = Dropout(0.5)(x) \n",
    "    x = Dense(1)(x)\n",
    "    model = Model([inp, inp2] , x)\n",
    "    \n",
    "    weights = [w for w in os.listdir('../input/osic-model-weights') if model_class in w][0]\n",
    "#     model.load_weights('../input/osic-model-weights/' + weights)\n",
    "    model.load_weights('../input/effnet-b5-30epochs-1/effnet_30.h5')\n",
    "    return model\n",
    "\n",
    "model_classes = ['b5'] #['b0','b1','b2','b3',b4','b5','b6','b7']\n",
    "models = [build_model(shape=(512, 512, 1), model_class=m) for m in model_classes]\n",
    "print('Number of models: ' + str(len(models)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split \n",
    "\n",
    "tr_p, vl_p = train_test_split(P, \n",
    "                              shuffle=True, \n",
    "                              train_size= 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.distplot(list(A.values()));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score(fvc_true, fvc_pred, sigma):\n",
    "    sigma_clip = np.maximum(sigma, 70) # changed from 70, trie 66.7 too\n",
    "    delta = np.abs(fvc_true - fvc_pred)\n",
    "    delta = np.minimum(delta, 1000)\n",
    "    sq2 = np.sqrt(2)\n",
    "    metric = (delta / sigma_clip)*sq2 + np.log(sigma_clip* sq2)\n",
    "    return np.mean(metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subs = []\n",
    "for model in models:\n",
    "\n",
    "    q = 0.5\n",
    "\n",
    "    sub = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/sample_submission.csv') \n",
    "    test = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv') \n",
    "    A_test, B_test, P_test,W, FVC= {}, {}, {},{},{} \n",
    "    STD, WEEK = {}, {} \n",
    "    for p in test.Patient.unique():\n",
    "        x = [] \n",
    "        tab = [] \n",
    "        ldir = os.listdir(f'../input/osic-pulmonary-fibrosis-progression/test/{p}/')\n",
    "        for i in ldir:\n",
    "            if int(i[:-4]) / len(ldir) < 1.1 and int(i[:-4]) / len(ldir) > -0.1:\n",
    "                x.append(get_img(f'../input/osic-pulmonary-fibrosis-progression/test/{p}/{i}')) \n",
    "                tab.append(get_tab(test.loc[test.Patient == p, :])) \n",
    "        if len(x) <= 1:\n",
    "            continue\n",
    "        tab = np.array(tab) \n",
    "\n",
    "        x = np.expand_dims(x, axis=-1) \n",
    "        _a = model.predict([x, tab]) \n",
    "        a = np.quantile(_a, q)\n",
    "        A_test[p] = a\n",
    "        B_test[p] = test.FVC.values[test.Patient == p] - a*test.Weeks.values[test.Patient == p]\n",
    "        P_test[p] = test.Percent.values[test.Patient == p] \n",
    "        WEEK[p] = test.Weeks.values[test.Patient == p]\n",
    "\n",
    "    for k in sub.Patient_Week.values:\n",
    "        p, w = k.split('_')\n",
    "        w = int(w) \n",
    "\n",
    "        fvc = A_test[p] * w + B_test[p]\n",
    "        sub.loc[sub.Patient_Week == k, 'FVC'] = fvc\n",
    "        sub.loc[sub.Patient_Week == k, 'Confidence'] = (\n",
    "            P_test[p] - A_test[p] * abs(WEEK[p] - w) \n",
    "    ) \n",
    "\n",
    "    _sub = sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()\n",
    "    subs.append(_sub)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Averaging Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(subs)\n",
    "sub = subs[0].copy() # ref\n",
    "sub[\"FVC\"] = 0\n",
    "sub[\"Confidence\"] = 0\n",
    "for i in range(N):\n",
    "    sub[\"FVC\"] += subs[0][\"FVC\"] * (1/N)\n",
    "    sub[\"Confidence\"] += subs[0][\"Confidence\"] * (1/N)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission_img.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_sub = sub[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Osic-Multiple-Quantile-Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT = \"../input/osic-pulmonary-fibrosis-progression\"\n",
    "BATCH_SIZE=128\n",
    "\n",
    "tr = pd.read_csv(f\"{ROOT}/train.csv\")\n",
    "tr.drop_duplicates(keep=False, inplace=True, subset=['Patient','Weeks'])\n",
    "chunk = pd.read_csv(f\"{ROOT}/test.csv\")\n",
    "\n",
    "print(\"add infos\")\n",
    "sub = pd.read_csv(f\"{ROOT}/sample_submission.csv\")\n",
    "sub['Patient'] = sub['Patient_Week'].apply(lambda x:x.split('_')[0])\n",
    "sub['Weeks'] = sub['Patient_Week'].apply(lambda x: int(x.split('_')[-1]))\n",
    "sub =  sub[['Patient','Weeks','Confidence','Patient_Week']]\n",
    "sub = sub.merge(chunk.drop('Weeks', axis=1), on=\"Patient\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr['WHERE'] = 'train'\n",
    "chunk['WHERE'] = 'val'\n",
    "sub['WHERE'] = 'test'\n",
    "data = tr.append([chunk, sub])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tr.shape, chunk.shape, sub.shape, data.shape)\n",
    "print(tr.Patient.nunique(), chunk.Patient.nunique(), sub.Patient.nunique(), \n",
    "      data.Patient.nunique())\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['min_week'] = data['Weeks']\n",
    "data.loc[data.WHERE=='test','min_week'] = np.nan\n",
    "data['min_week'] = data.groupby('Patient')['min_week'].transform('min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = data.loc[data.Weeks == data.min_week]\n",
    "base = base[['Patient','FVC']].copy()\n",
    "base.columns = ['Patient','min_FVC']\n",
    "base['nb'] = 1\n",
    "base['nb'] = base.groupby('Patient')['nb'].transform('cumsum')\n",
    "base = base[base.nb==1]\n",
    "base.drop('nb', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.merge(base, on='Patient', how='left')\n",
    "data['base_week'] = data['Weeks'] - data['min_week']\n",
    "del base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLS = ['Sex','SmokingStatus'] #,'Age'\n",
    "FE = []\n",
    "for col in COLS:\n",
    "    for mod in data[col].unique():\n",
    "        FE.append(mod)\n",
    "        data[mod] = (data[col] == mod).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "data['age'] = (data['Age'] - data['Age'].min() ) / ( data['Age'].max() - data['Age'].min() )\n",
    "data['BASE'] = (data['min_FVC'] - data['min_FVC'].min() ) / ( data['min_FVC'].max() - data['min_FVC'].min() )\n",
    "data['week'] = (data['base_week'] - data['base_week'].min() ) / ( data['base_week'].max() - data['base_week'].min() )\n",
    "#data['percent'] = (data['Percent'] - data['Percent'].min() ) / ( data['Percent'].max() - data['Percent'].min() )\n",
    "FE += ['age','week','BASE']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = data.loc[data.WHERE=='train']\n",
    "chunk = data.loc[data.WHERE=='val']\n",
    "sub = data.loc[data.WHERE=='test']\n",
    "del data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr.shape, chunk.shape, sub.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "C1, C2 = tf.constant(70, dtype='float32'), tf.constant(1000, dtype=\"float32\")\n",
    "\n",
    "def score(y_true, y_pred):\n",
    "    tf.dtypes.cast(y_true, tf.float32)\n",
    "    tf.dtypes.cast(y_pred, tf.float32)\n",
    "    sigma = y_pred[:, 2] - y_pred[:, 0]\n",
    "    fvc_pred = y_pred[:, 1]\n",
    "    \n",
    "    #sigma_clip = sigma + C1\n",
    "    sigma_clip = tf.maximum(sigma, C1)\n",
    "    delta = tf.abs(y_true[:, 0] - fvc_pred)\n",
    "    delta = tf.minimum(delta, C2)\n",
    "    sq2 = tf.sqrt( tf.dtypes.cast(2, dtype=tf.float32) )\n",
    "    metric = (delta / sigma_clip)*sq2 + tf.math.log(sigma_clip* sq2)\n",
    "    return K.mean(metric)\n",
    "\n",
    "def qloss(y_true, y_pred):\n",
    "    # Pinball loss for multiple quantiles\n",
    "    qs = [0.2, 0.50, 0.8]\n",
    "    q = tf.constant(np.array([qs]), dtype=tf.float32)\n",
    "    e = y_true - y_pred\n",
    "    v = tf.maximum(q*e, (q-1)*e)\n",
    "    return K.mean(v)\n",
    "\n",
    "def mloss(_lambda):\n",
    "    def loss(y_true, y_pred):\n",
    "        return _lambda * qloss(y_true, y_pred) + (1 - _lambda)*score(y_true, y_pred)\n",
    "    return loss\n",
    "\n",
    "def make_model(nh):\n",
    "    z = L.Input((nh,), name=\"Patient\")\n",
    "    x = L.Dense(100, activation=\"relu\", name=\"d1\")(z)\n",
    "    x = L.Dense(100, activation=\"relu\", name=\"d2\")(x)\n",
    "    #x = L.Dense(100, activation=\"relu\", name=\"d3\")(x)\n",
    "    p1 = L.Dense(3, activation=\"linear\", name=\"p1\")(x)\n",
    "    p2 = L.Dense(3, activation=\"relu\", name=\"p2\")(x)\n",
    "    preds = L.Lambda(lambda x: x[0] + tf.cumsum(x[1], axis=1), \n",
    "                     name=\"preds\")([p1, p2])\n",
    "    \n",
    "    model = M.Model(z, preds, name=\"CNN\")\n",
    "    #model.compile(loss=qloss, optimizer=\"adam\", metrics=[score])\n",
    "    model.compile(loss=mloss(0.8), optimizer=tf.keras.optimizers.Adam(lr=0.1, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.01, amsgrad=False), metrics=[score])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tr['FVC'].values\n",
    "z = tr[FE].values\n",
    "ze = sub[FE].values\n",
    "nh = z.shape[1]\n",
    "pe = np.zeros((ze.shape[0], 3))\n",
    "pred = np.zeros((z.shape[0], 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = make_model(nh)\n",
    "print(net.summary())\n",
    "print(net.count_params())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "NFOLD = 2 # originally 5\n",
    "kf = KFold(n_splits=NFOLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "cnt = 0\n",
    "EPOCHS = 600\n",
    "for tr_idx, val_idx in kf.split(z):\n",
    "    cnt += 1\n",
    "    print(f\"FOLD {cnt}\")\n",
    "    net = make_model(nh)\n",
    "    net.fit(z[tr_idx], y[tr_idx], batch_size=BATCH_SIZE, epochs=EPOCHS, \n",
    "            validation_data=(z[val_idx], y[val_idx]), verbose=0) #\n",
    "    print(\"train\", net.evaluate(z[tr_idx], y[tr_idx], verbose=0, batch_size=BATCH_SIZE))\n",
    "    print(\"val\", net.evaluate(z[val_idx], y[val_idx], verbose=0, batch_size=BATCH_SIZE))\n",
    "    print(\"predict val...\")\n",
    "    pred[val_idx] = net.predict(z[val_idx], batch_size=BATCH_SIZE, verbose=0)\n",
    "    print(\"predict test...\")\n",
    "    pe += net.predict(ze, batch_size=BATCH_SIZE, verbose=0) / NFOLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigma_opt = mean_absolute_error(y, pred[:, 1])\n",
    "unc = pred[:,2] - pred[:, 0]\n",
    "sigma_mean = np.mean(unc)\n",
    "print(sigma_opt, sigma_mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idxs = np.random.randint(0, y.shape[0], 100)\n",
    "plt.plot(y[idxs], label=\"ground truth\")\n",
    "plt.plot(pred[idxs, 0], label=\"q25\")\n",
    "plt.plot(pred[idxs, 1], label=\"q50\")\n",
    "plt.plot(pred[idxs, 2], label=\"q75\")\n",
    "plt.legend(loc=\"best\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(unc.min(), unc.mean(), unc.max(), (unc>=0).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(unc)\n",
    "plt.title(\"uncertainty in prediction\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PREDICTION\n",
    "sub['FVC1'] = 1.*pe[:, 1]\n",
    "sub['Confidence1'] = pe[:, 2] - pe[:, 0]\n",
    "subm = sub[['Patient_Week','FVC','Confidence','FVC1','Confidence1']].copy()\n",
    "subm.loc[~subm.FVC1.isnull()].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.loc[~subm.FVC1.isnull(),'FVC'] = subm.loc[~subm.FVC1.isnull(),'FVC1']\n",
    "sigma_mean = 60\n",
    "if sigma_mean<sigma_mean:\n",
    "    subm['Confidence'] = sigma_opt\n",
    "else:\n",
    "    subm.loc[~subm.FVC1.isnull(),'Confidence'] = subm.loc[~subm.FVC1.isnull(),'Confidence1']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "otest = pd.read_csv('../input/osic-pulmonary-fibrosis-progression/test.csv')\n",
    "for i in range(len(otest)):\n",
    "    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'FVC'] = otest.FVC[i]\n",
    "    subm.loc[subm['Patient_Week']==otest.Patient[i]+'_'+str(otest.Weeks[i]), 'Confidence'] = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].to_csv(\"submission_regression.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_sub = subm[[\"Patient_Week\",\"FVC\",\"Confidence\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble (Simple Blend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = img_sub.sort_values(by=['Patient_Week'], ascending=True).reset_index(drop=True)\n",
    "df2 = reg_sub.sort_values(by=['Patient_Week'], ascending=True).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df1[['Patient_Week']].copy()\n",
    "df['FVC'] = (0.45*df1['FVC'] + 0.55*df2['FVC'])\n",
    "df['Confidence'] = (0.45*df1['Confidence'] + 0.55*df2['Confidence'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 1357052,
     "sourceId": 20604,
     "sourceType": "competition"
    },
    {
     "datasetId": 642364,
     "sourceId": 1139701,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 807680,
     "sourceId": 1383977,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 774060,
     "sourceId": 1401932,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 820718,
     "sourceId": 1407014,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 906925,
     "sourceId": 1537895,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 29987,
   "isGpuEnabled": true,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
